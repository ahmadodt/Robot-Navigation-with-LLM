# Robot Navigation with LLM

This project simulates a simple robot navigating a grid using instructions generated by a Large Language Model (LLM). The LLM generates commands based on the robot's current position, the positions of obstacles, and the goal position. The interaction is achieved using text prompts rather than traditional matrices.

## Table of Contents

- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
- [Configuration](#configuration)
- [Project Structure](#project-structure)
- [Future Improvements](#future-improvements)

## Features

- **Robot Simulation**: A grid-based simulation of a robot moving towards a goal.
- **LLM Integration**: Uses an LLM to interpret the robot's surroundings and generate movement commands.
- **Configurable Settings**: Easily modify grid size, obstacles, and other parameters using JSON configuration files.

## Installation

### Clone the repository

```bash
git clone https://github.com/yourusername/robot-navigation-llm.git
cd robot-navigation-llm
```
### Install Dependencies

Make sure you have Python installed. Install required packages using pip:

pip install -r requirements.txt

## Usage

### Configure Settings

Modify the config.json, prompts.json, and few_shots.json files in the json_files directory to customize the robot's behavior and the environment. (IF YOU WANT NOT NECESSARY)

### Run the Simulation

Use the following command to start the robot navigation simulation:

python main.py

### View Output

The robot's movements and commands will be output to the console and stored in json_files/command_output.json.

## Configuration

- **config.json**: Contains configuration for the LLM, initial robot position, grid map, goal position, and the number of moves.
- **prompts.json**: Provides the templates for prompts used to instruct the LLM based on different scenarios.
- **few_shots.json**: Contains examples of prompts and expected results for training or few-shot learning scenarios.

### Example Configuration (`config.json`)

```json
{
    "TOKEN": "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",
    "MODEL_NAME": "meta-llama/Llama-2-7b-chat-hf",
    "FEW_SHOTS_FILE": "json_files/few_shots.json",
    "PROMPT_TEMPLATES": "json_files/prompts.json",
    "COMMAND_OUTPUT_FILE": "json_files/command_output.json",
    "MAP_MATRIX": [
        [0, 2, 0, 0, 3, 0],
        [0, 0, 0, 2, 0, 0],
        [0, 0, 1, 0, 0, 3],
        [3, 0, 0, 0, 0, 0],
        [0, 0, 2, 0, 3, 0],
        [0, 0, 0, 0, 0, 2]
    ],
    "GOAL_POSITION": [4, 5],
    "MOVES": 4,
    "NUM_FEW_SHOTS": 3
}
```

## Project Structure

- **main.py**: The main script to run the simulation.
- **prompter.py**: Contains classes and methods to handle prompt generation for the LLM.
- **utils.py**: Utility functions and classes used across the project.
- **json_files/**: Directory containing JSON configuration and prompt templates.

## Future Improvements

- **Enhanced LLM Feedback**: Integrate feedback mechanisms for the LLM to learn and adapt from the robot's performance.
- **Obstacle Types**: Expand the range of obstacles and environmental factors the LLM can interpret.
- **Visualization**: Implement a GUI or a visualization tool to better observe the robot's movements and decisions.
